# -*- coding: utf-8 -*-
"""week5

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MLF2ZJ_eIe4Jpmof46bbkrFwPsuk_ntN
"""

# =======================
# Colab setup & imports
# =======================
!pip install -q transformers accelerate datasets

import os
import torch
import random
import numpy as np
from datasets import load_dataset
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM

# =======================
# Config
# =======================
MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"  # [web:19]
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
LEARNING_RATE = 1e-5
EPOCHS = 1           # keep small for Colab
DEBUG_SMALL = True   # True => use tiny subsets so it actually runs
SEED = 42
N_OFFLINE_SAMPLES = 2  # number of samples per question for V* estimation

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

print(f"Using device: {DEVICE}")

# =======================
# Load tokenizer & models
# =======================
print(f"Loading model {MODEL_NAME} ...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Trainable model (policy)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32,
    device_map="auto" if DEVICE == "cuda" else None,
)
model.to(DEVICE)
model.train()

# Reference model (for offline V* estimation)
ref_model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32,
    device_map="auto" if DEVICE == "cuda" else None,
)
ref_model.to(DEVICE)
ref_model.eval()

# =======================
# Data loading
# =======================
def load_math_datasets(debug_small=False):
    """
    Train: EleutherAI/hendrycks_math (algebra subset as a proxy). [web:26]
    Eval:  HuggingFaceH4/MATH-500. [web:27]
    """
    print("Loading datasets from Hugging Face...")
    math_ds = load_dataset("EleutherAI/hendrycks_math", "algebra", split="train")  # [web:26]
    math500 = load_dataset("HuggingFaceH4/MATH-500", split="test")                 # [web:27]

    def extract(dataset, limit=None):
        items = []
        for i, ex in enumerate(dataset):
            q = ex.get("problem") or ex.get("question") or ""
            ans = ex.get("solution") or ex.get("answer") or ""
            if q and ans:
                items.append((q.strip(), ans.strip()))
            if limit and i + 1 >= limit:
                break
        return items

    # Tiny subsets for debugging
    train_limit = 50 if debug_small else None
    eval_limit = 20 if debug_small else None

    train_list = extract(math_ds, limit=train_limit)
    eval_list = extract(math500, limit=eval_limit)

    print(f"Loaded {len(train_list)} training examples and {len(eval_list)} eval examples.")
    return train_list, eval_list

# =======================
# Generation helpers
# =======================
@torch.no_grad()
def generate_with_model(model_obj, question: str, max_new_tokens=128, sample=True):
    """
    Generate an answer to a question using the given model.
    """
    input_text = f"Question: {question}\nAnswer:"
    inputs = tokenizer(input_text, return_tensors="pt").to(DEVICE)
    gen_kwargs = {
        "max_new_tokens": max_new_tokens,
        "do_sample": sample,
        "top_p": 0.9,
        "temperature": 0.7,
        "pad_token_id": tokenizer.eos_token_id,
    }
    outputs = model_obj.generate(**inputs, **gen_kwargs)
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    if "Answer:" in result:
        result = result.split("Answer:")[-1]
    return result.strip()

def compute_reward(pred: str, gold: str) -> float:
    """
    Very simple reward: 1.0 if gold answer appears as substring in prediction; 0.0 otherwise.
    This is only a proxy; MATH answers often need more careful parsing. [web:26][web:27]
    """
    return 1.0 if gold.lower().strip() in pred.lower() else 0.0

# =======================
# Offline V* estimation
# =======================
def estimate_v_star(train_data, n_samples=2):
    """
    For each question, sample n trajectories from ref_model and store max reward as V*(x). [web:4]
    """
    v_star = {}
    for q, a in tqdm(train_data, desc="Estimating V*"):
        rewards = []
        for _ in range(n_samples):
            pred = generate_with_model(ref_model, q, sample=True)
            r = compute_reward(pred, a)
            rewards.append(r)
        v_star[q] = max(rewards) if rewards else 0.0
    return v_star

# =======================
# Log-prob computation
# =======================
def compute_generated_logprob(inputs, generated_ids):
    """
    Compute log-prob of generated tokens under the current policy model.
    """
    with torch.no_grad():
        prompt_len = inputs["input_ids"].shape[1]

    # Concatenate prompt and generated ids
    full_ids = torch.cat([inputs["input_ids"], generated_ids], dim=1)

    # Compute logits with labels for convenience
    outputs = model(full_ids, labels=full_ids)
    logits = outputs.logits  # [1, T, vocab]
    shift_labels = full_ids[:, 1:]
    logits = logits[:, :-1, :]  # shift for next-token prediction

    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)
    token_log_probs = log_probs.gather(-1, shift_labels.unsqueeze(-1)).squeeze(-1)
    # token_log_probs: [1, T-1]

    # We want only the generated tokens, not the prompt
    # Prompt tokens occupy positions [0, prompt_len-1]; their logprobs end at prompt_len-2.
    # The first generated token logprob is at index (prompt_len-1).
    gen_log_probs = token_log_probs[:, prompt_len - 1 :]
    return gen_log_probs.sum(dim=-1)  # shape [1]

# =======================
# A*-PO single train step
# =======================
def a_star_po_train_step(question, answer, v_star_value, optimizer, max_new_tokens=128):
    """
    One A*-PO update:
      1) Sample trajectory from current policy.
      2) Compute reward r and advantage A = r - V*(x).
      3) Update policy via -A * log pi. [web:4]
    """
    # Prepare prompt
    input_text = f"Question: {question}\nAnswer:"
    inputs = tokenizer(input_text, return_tensors="pt").to(DEVICE)

    # Sample trajectory from current policy
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        top_p=0.9,
        temperature=0.7,
        pad_token_id=tokenizer.eos_token_id,
    )
    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    if "Answer:" in full_text:
        pred = full_text.split("Answer:")[-1].strip()
    else:
        pred = full_text.strip()

    # Reward and advantage
    r = compute_reward(pred, answer)
    A = r - v_star_value

    # Optionally, skip near-zero advantages
    if abs(A) < 1e-6:
        return float(A), float(r)

    # Extract generated token ids (excluding prompt)
    gen_ids = outputs[:, inputs["input_ids"].shape[1]:]
    logp = compute_generated_logprob(inputs, gen_ids)  # [1]

    # A*-PO loss: maximize A * log pi  <=> minimize -A * log pi. [web:4]
    loss = -A * logp.mean()

    optimizer.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
    optimizer.step()

    return float(A), float(r)

# =======================
# Training loop
# =======================
def train_model_a_star_po(train_data, v_star, epochs=1):
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    for epoch in range(epochs):
        total_adv = 0.0
        total_r = 0.0
        for q, a in tqdm(train_data, desc=f"A*-PO Epoch {epoch+1}/{epochs}"):
            v = v_star.get(q, 0.0)
            A, r = a_star_po_train_step(q, a, v, optimizer)
            total_adv += A
            total_r += r
        avg_adv = total_adv / len(train_data)
        avg_r = total_r / len(train_data)
        print(f"Epoch {epoch+1}: avg advantage={avg_adv:.4f}, avg reward={avg_r:.4f}")

# =======================
# Evaluation (simple accuracy)
# =======================
def generate_answer(question: str, max_new_tokens=128):
    return generate_with_model(model, question, max_new_tokens=max_new_tokens, sample=False)

def evaluate_model(eval_data):
    model.eval()
    correct = 0
    total = len(eval_data)
    for q, gold in tqdm(eval_data, desc="Evaluating"):
        pred = generate_answer(q)
        if gold.lower().strip() in pred.lower():
            correct += 1
    acc = 100 * correct / total
    print(f"Evaluation Accuracy: {acc:.2f}% ({correct}/{total})")
    model.train()
    return acc

# =======================
# Main
# =======================
def main():
    # Load data
    train_set, eval_set = load_math_datasets(debug_small=DEBUG_SMALL)

    # Offline V* estimation (on same train subset used for RL)
    print("\nEstimating V* offline ...")
    train_subset = train_set[:50] if DEBUG_SMALL else train_set
    v_star = estimate_v_star(train_subset, n_samples=N_OFFLINE_SAMPLES)

    # Baseline accuracy
    print("\nEvaluating before RL training ...")
    eval_subset = eval_set[:20] if DEBUG_SMALL else eval_set
    baseline_acc = evaluate_model(eval_subset)

    # RL training with A*-PO
    print("\nTraining model with A*-PO ...")
    train_model_a_star_po(train_subset, v_star, epochs=EPOCHS)

    # Post-training accuracy
    print("\nEvaluating after RL training ...")
    post_acc = evaluate_model(eval_subset)

    print("\nSummary:")
    print(f"Before Training: {baseline_acc:.2f}%")
    print(f"After Training:  {post_acc:.2f}%")

if __name__ == "__main__":
    main()